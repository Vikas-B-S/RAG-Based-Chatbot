{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54c15b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from faiss-cpu) (2.3.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (0.3.29)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from langchain-community) (0.3.76)\n",
      "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from langchain-community) (0.3.27)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from langchain-community) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2.32.5 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from langchain-community) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from langchain-community) (0.4.29)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=2.1.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from langchain-community) (2.3.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.11)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (2.11.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from requests<3,>=2.32.5->langchain-community) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from requests<3,>=2.32.5->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from requests<3,>=2.32.5->langchain-community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from requests<3,>=2.32.5->langchain-community) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from langsmith>=0.1.125->langchain-community) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from langsmith>=0.1.125->langchain-community) (0.25.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from sentence-transformers) (4.56.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from sentence-transformers) (0.35.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vikas b s\\onedrive\\desktop\\great learning\\genai_case_studies\\rag_based_chatbox\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "#Requirements\n",
    "!pip install faiss-cpu\n",
    "!pip install langchain-community\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01e8ad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from langchain.vectorstores import FAISS # This will be the vector database\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings # To perform word embeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # This is for chunking\n",
    "from pypdf import PdfReader\n",
    "import faiss "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01279f82",
   "metadata": {},
   "source": [
    "## Step 1: Configure the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f90f4a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LLM\n",
    "key  = os.getenv('GOOGLE_API_KEY')\n",
    "genai.configure(api_key = key)\n",
    "llm_model = genai.GenerativeModel('gemini-2.5-flash-lite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7242bd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Embedding Model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06cdc0d",
   "metadata": {},
   "source": [
    "## Step 2: Loading the PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d593535",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_file = PdfReader('RAG_CHATBOT.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca960e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = ''\n",
    "for page in loaded_file.pages:\n",
    "    text_only = page.extract_text()\n",
    "    if text_only:\n",
    "        raw_text += text_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b4c88c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Study: RAG Chatbot Powered by Google \n",
      "Gemini for Smart Document Q&A \n",
      "Project Title: Intelligent Document Q&A Assistant using Retrieval-Augmented Generation \n",
      "(RAG) with Gemini \n",
      "GitHub Repository: https://github.com/mukul-mschauhan/RAG-Using-Gemini \n",
      "Live Demo: https://gemini-rag2025.streamlit.app/ \n",
      " \n",
      "Problem Statement \n",
      "Across industries such as legal, finance, healthcare, and construction, professionals are \n",
      "required to extract insights from massive document repositories—contracts, product \n",
      "manuals, policies, reports, regulations, and emails. \n",
      "Traditional keyword-based search and static FAQs fail to deliver contextual, accurate \n",
      "answers. Employees waste hours scanning PDFs and notes, leading to operational \n",
      "inefficiencies, poor decision-making, and knowledge silos. \n",
      "There’s a critical need for an intelligent assistant that can understand natural language \n",
      "questions, reason over domain-specific documents, and deliver precise responses—\n",
      "instantly. \n",
      " \n",
      "Business Objective \n",
      "To build an enterprise-grade, scalable, and cost-efficient RAG-powered AI assistant that \n",
      "enables: \n",
      " Smart retrieval and summarization of large-scale PDF/text content \n",
      " Natural language understanding of user queries \n",
      " Domain-specific knowledge augmentation via vector search \n",
      " Real-time Q&A over user-uploaded documents \n",
      "This tool is aimed at: - Legal & Compliance teams automating contract review - Analysts \n",
      "querying internal policy documents - Researchers navigating technical papers - Support \n",
      "teams handling customer manuals and SOPs \n",
      " Proposed Solution \n",
      "We built a fully functional web application that allows users to: \n",
      "1. Upload one or more PDF/text documents \n",
      "2. Extract content and embed it using state-of-the-art vectorization (Google Gemini-\n",
      "compatible) \n",
      "3. Store and retrieve relevant chunks using vector search (FAISS) \n",
      "4. Ask questions in natural language \n",
      "5. Get contextual answers generated by Google Gemini 1.5 Flash using the retrieved \n",
      "documents \n",
      " \n",
      "Architecture Overview \n",
      "1. Frontend: Streamlit web UI for uploading files and chat interface \n",
      "2. Document Processing: Text extraction using PyMuPDF and chunking logic \n",
      "3. Embeddings: Google-compatible embeddings (e.g., Gemini/Vertex-compatible or \n",
      "from SentenceTransformers) \n",
      "4. Vector Store: FAISS for similarity search on embedded text chunks \n",
      "5. LLM Integration: Google Gemini 1.5 Flash for natural language generation using \n",
      "retrieved chunks as context \n",
      "6. Prompt Engineering: Carefully crafted system and user prompts to ensure \n",
      "contextual relevance and safety \n",
      " \n",
      "Tech Stack \n",
      "Layer Tools Used \n",
      "LLM Google Gemini 1.5 Flash \n",
      "Vector DB FAISS \n",
      "Embeddings SentenceTransformers / Gemini \n",
      "Text Extraction PyMuPDF (fitz) \n",
      "Frontend Streamlit \n",
      "Backend Integration LangChain \n",
      "Deployment Streamlit Cloud \n",
      "Programming Language Python \n",
      " \n",
      "Business Impact \n",
      " Reduced document navigation time by 90% \n",
      " Enabled 24x7 AI assistant for contract, policy, and research Q&A  Democratized document access for non-technical users \n",
      " Scalable for internal or customer-facing use cases \n",
      "Example Use Cases: - Ask a policy question like: “What is the refund timeline for \n",
      "cancelled trips?” - Upload 5 contracts and ask: “Which clause discusses penalty on late \n",
      "delivery?” - Upload medical SOPs and ask: “When to escalate Stage 2 hypertension?” \n",
      " \n",
      "Conclusion & Future Roadmap \n",
      "This RAG solution bridges the gap between static document stores and intelligent, real-\n",
      "time assistance. \n",
      "Next Enhancements: - Multi-user support with user-based document history - Support for \n",
      "audio documents via speech-to-text - API endpoints for enterprise SaaS integration - In-\n",
      "built analytics dashboard \n",
      " \n",
      "For the complete implementation, visit the GitHub repo: 跚跛跜距 https://github.com/mukul-\n",
      "mschauhan/RAG-Using-Gemini \n",
      "Try the live chatbot: 뜜뜝뜡뜢뜞뜟뜠 https://gemini-rag2025.streamlit.app/ \n"
     ]
    }
   ],
   "source": [
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d089f7",
   "metadata": {},
   "source": [
    "## Step 3: Chunking (Create Chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ba6a36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 300, chunk_overlap = 50)\n",
    "chunks = splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "359169f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b0c41c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Study: RAG Chatbot Powered by Google \n",
      "Gemini for Smart Document Q&A \n",
      "Project Title: Intelligent Document Q&A Assistant using Retrieval-Augmented Generation \n",
      "(RAG) with Gemini \n",
      "GitHub Repository: https://github.com/mukul-mschauhan/RAG-Using-Gemini\n"
     ]
    }
   ],
   "source": [
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebb037a",
   "metadata": {},
   "source": [
    "## Step 4: Create FAISS Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0dd6d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_texts(chunks, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415dc5fc",
   "metadata": {},
   "source": [
    "## Step 5: Configure Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afa516a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={'k':3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf1e8aa",
   "metadata": {},
   "source": [
    "## Step 6: Take the Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c8d2269",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Show me the steps to proceed with this project.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bcca85",
   "metadata": {},
   "source": [
    "## Step 7: Retrieval (R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3c3ff6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_25256\\209730803.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrived_documents = retriever.get_relevant_documents(query=query)\n"
     ]
    }
   ],
   "source": [
    "retrived_documents = retriever.get_relevant_documents(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9d7c569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Live Demo: https://gemini-rag2025.streamlit.app/ \\n \\nProblem Statement \\nAcross industries such as legal, finance, healthcare, and construction, professionals are \\nrequired to extract insights from massive document repositories—contracts, product \\nmanuals, policies, reports, regulations, and emails. 4. Ask questions in natural language \\n5. Get contextual answers generated by Google Gemini 1.5 Flash using the retrieved \\ndocuments \\n \\nArchitecture Overview \\n1. Frontend: Streamlit web UI for uploading files and chat interface Case Study: RAG Chatbot Powered by Google \\nGemini for Smart Document Q&A \\nProject Title: Intelligent Document Q&A Assistant using Retrieval-Augmented Generation \\n(RAG) with Gemini \\nGitHub Repository: https://github.com/mukul-mschauhan/RAG-Using-Gemini'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = ' '.join([doc.page_content for doc in retrived_documents])\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46522acf",
   "metadata": {},
   "source": [
    "## Step 8: Write a Augmentation Prompt (A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "456e7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'''You are a helpful assistant using RAG\n",
    "Here is the {context}\n",
    "\n",
    "The query asked by the user is as follows = {query}'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522003e8",
   "metadata": {},
   "source": [
    "## Step 9: Generation (G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75bea2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the steps to proceed with the Intelligent Document Q&A Assistant project, based on the provided information:\n",
      "\n",
      "**1. Project Setup and Environment:**\n",
      "\n",
      "*   **Clone the Repository:** Start by cloning the GitHub repository: `https://github.com/mukul-mschauhan/RAG-Using-Gemini` to your local machine.\n",
      "*   **Set up a Python Environment:** It's highly recommended to use a virtual environment (e.g., `venv` or `conda`) to manage project dependencies.\n",
      "*   **Install Dependencies:** Install all necessary Python packages listed in the project's `requirements.txt` file. You can usually do this with `pip install -r requirements.txt`.\n",
      "\n",
      "**2. Understanding the Core Components:**\n",
      "\n",
      "*   **Frontend (Streamlit):**\n",
      "    *   Familiarize yourself with the Streamlit code. This will handle:\n",
      "        *   User interface for file uploads (documents).\n",
      "        *   The chat interface for asking questions.\n",
      "        *   Displaying the generated answers.\n",
      "*   **Backend (RAG - Retrieval-Augmented Generation):**\n",
      "    *   **Document Ingestion and Processing:** Understand how documents are loaded, parsed, and potentially chunked into smaller pieces for easier processing.\n",
      "    *   **Embedding Generation:** Learn how the text chunks are converted into numerical vector representations (embeddings) using a model.\n",
      "    *   **Vector Database/Store:** Identify how these embeddings are stored and indexed for efficient retrieval. This could be a dedicated vector database or an in-memory solution.\n",
      "    *   **Retrieval Mechanism:** Understand the logic for searching the vector store based on the user's query embedding to find the most relevant document chunks.\n",
      "    *   **Gemini Integration (Google Gemini 1.5 Flash):**\n",
      "        *   This is the core of the generation part. You'll need to understand how the retrieved document chunks are passed as context to the Gemini model.\n",
      "        *   Learn how Gemini is prompted to generate a contextual answer based on the provided context and the user's question.\n",
      "\n",
      "**3. Key Development and Configuration Steps:**\n",
      "\n",
      "*   **Gemini API Key:** You will need to obtain an API key for Google Gemini. This will likely involve setting up a Google Cloud project and enabling the Gemini API.\n",
      "*   **Configuration:** Identify any configuration files or environment variables where you need to store your Gemini API key and potentially other settings (e.g., model parameters, chunking strategies).\n",
      "*   **Data Loading and Chunking:** Implement or adapt the code responsible for loading various document formats (PDF, DOCX, TXT, etc.) and breaking them down into manageable text chunks.\n",
      "*   **Embedding Model Selection:** Understand which embedding model is being used (e.g., from the Hugging Face `transformers` library or a specific Google embedding model).\n",
      "*   **Vector Store Implementation:** Verify the chosen vector store and how it's being interacted with.\n",
      "*   **Streamlit UI Development/Customization:**\n",
      "    *   If you need to modify the UI, understand Streamlit's layout, widgets, and state management.\n",
      "    *   Ensure the file upload functionality is robust.\n",
      "    *   The chat input and output display should be intuitive.\n",
      "\n",
      "**4. Running the Project:**\n",
      "\n",
      "*   **Local Execution:**\n",
      "    *   Navigate to the project directory in your terminal.\n",
      "    *   Run the Streamlit application using the command: `streamlit run your_app_file.py` (replace `your_app_file.py` with the actual name of your main Streamlit script, likely `app.py` or similar).\n",
      "*   **Deployment (for the live demo):**\n",
      "    *   The live demo suggests a deployment. If you plan to deploy your project (e.g., to Streamlit Cloud, Heroku, or a custom server), you'll need to:\n",
      "        *   Configure your deployment environment.\n",
      "        *   Ensure all dependencies are correctly installed.\n",
      "        *   Manage your API keys securely in the deployment environment.\n",
      "\n",
      "**5. Testing and Iteration:**\n",
      "\n",
      "*   **Upload Diverse Documents:** Test with a variety of document types and sizes to ensure the ingestion and processing are working correctly.\n",
      "*   **Ask Varied Questions:** Experiment with different types of questions:\n",
      "    *   Factual questions (e.g., \"What is the deadline for X?\").\n",
      "    *   Summarization questions (e.g., \"Summarize the main points of this contract.\").\n",
      "    *   Comparison questions (e.g., \"What are the differences between policy A and policy B?\").\n",
      "*   **Evaluate Answers:** Assess the accuracy, relevance, and conciseness of the answers generated by Gemini.\n",
      "*   **Tune Parameters:** If needed, adjust parameters related to document chunking, embedding similarity thresholds, or Gemini's generation settings to improve performance.\n",
      "\n",
      "By following these steps, you should be able to understand, set up, and run the Intelligent Document Q&A Assistant project. Remember to refer to the specific code within the GitHub repository for detailed implementation specifics.\n"
     ]
    }
   ],
   "source": [
    "print(llm_model.generate_content(prompt).text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
